#!/usr/bin/env python3

import argparse
import difflib
import itertools
import json
import re
import subprocess

from os import listdir, path

CURRENT_DIR = path.dirname(path.realpath(__file__))

KERMIT_OUTPUT_PARSER_REGEX = re.compile("([0-9a-f]+) - ([a-z\/\-]+) - \"(.*)\" - Offset ([0-9]+)")

IGNORED_METADATA_KEYS = [
    'contentType',
    'license',
    'outgoingLinks',
    'temporalCoverage',
]

COLORS = {
    'cyan': '\033[36m',
    'green': '\033[32m',
    'red': '\033[31m',
    'reset': '\033[0m',
}

def _get_shards(shard_dir):
    shards = []

    for filename in listdir(shard_dir):
        full_filename = path.join(shard_dir, filename)
        if not path.isfile(path.join(shard_dir, filename)):
            continue

        if not filename.endswith('.shard'):
            continue

        shards.append(full_filename)

    return sorted(shards)

def parse_shard(assets, shard_path):
    shard_short_name = path.splitext(path.basename(shard_path))[0]
    shard_raw_items = subprocess.check_output(['kermit', 'list', shard_path]).decode('utf8') \
                                                                             .splitlines()
    articles = 0
    for raw_asset in shard_raw_items:
        parsed_asset = KERMIT_OUTPUT_PARSER_REGEX.search(raw_asset)

        title = parsed_asset.group(3)
        mime_type = parsed_asset.group(2)

        asset = {
            'ekn_id': parsed_asset.group(1),
            'mime_type': mime_type,
            'offset': parsed_asset.group(4),
            'shard': shard_short_name,
            'title': title,
        }

        assets[title] = assets.get(title, []) + [asset]

        if mime_type == 'text/html':
            articles += 1

    print(' - {} ({} assets, {} articles)'.format(shard_short_name,
                                                  len(shard_raw_items),
                                                  articles))

    return articles

def get_duplicate_articles(assets):
    for article_assets in assets.values():
        articles = [asset for asset in article_assets if asset['mime_type'] == 'text/html']

        if len(articles) > 1:
            yield articles

def get_metadata(shard_dir, shard_id, ekn_id):
    kermit_output = subprocess.check_output(['kermit',
                                             'dump',
                                             path.join(shard_dir, '{}.shard'.format(shard_id)),
                                             ekn_id,
                                             'metadata']).decode('utf8')

    metadata_object = json.loads(kermit_output)
    metadata_object['shard_id'] = shard_id

    for ignored_key in IGNORED_METADATA_KEYS:
        del metadata_object[ignored_key]

    return metadata_object

def print_color_diff(line):
    color_prefix = COLORS['reset']
    if line.startswith('+'):
        color_prefix = COLORS['green']
    elif line.startswith('-'):
        color_prefix = COLORS['red']
    elif line.startswith('@@ '):
        color_prefix = COLORS['cyan']

    print('{}{}{}'.format(color_prefix, line, COLORS['reset']))

def process_duplicates(shard_dir):
    print('Examining shard dir: {}'.format(shard_dir))

    shards = _get_shards(shard_dir)
    print('Found shards:')
    for shard in shards:
        print(' - {}'.format(shard))
    print()

    assets = {}
    total_articles = 0

    print('Collecting items in the shards...')
    for shard in shards:
        # Note: Assets is mutated in the call
        total_articles += parse_shard(assets, shard)
    print()

    total_unique_articles = len(assets)
    print('Unique articles found: {}'.format(total_unique_articles))
    print('Unique articles expected: {}'.format(total_articles))
    print()

    # If no duplicates, exit out early
    if total_articles == total_unique_articles:
        print('No duplicate articles found!')
        return

    # Otherwise, assume that we have duplicates
    print('WARNING! There seems to be {} duplicate articles in these shards!'.format(
        total_articles - total_unique_articles))
    print()

    for duplicate_article in get_duplicate_articles(assets):
        title = duplicate_article[0]['title']
        print('Duplicate ({}x): "{}"'.format(len(duplicate_article), title))
        print()

        metadatas = []
        for article_asset in duplicate_article:
            shard = article_asset['shard']
            ekn_id = article_asset['ekn_id']
            print(' - Shard: {}, Ekn ID: {}'.format(shard, ekn_id))

            metadatas.append(get_metadata(shard_dir, shard, ekn_id))

        print()

        for combination in itertools.combinations(metadatas, 2):
            obj1, obj2 = combination
            print(' Comparing {} with {}'.format(obj1['@id'], obj2['@id']))

            obj1_text = json.dumps(obj1, indent=4, sort_keys=True).splitlines()
            obj2_text = json.dumps(obj2, indent=4, sort_keys=True).splitlines()

            for line in difflib.unified_diff(obj1_text, obj2_text):
                print_color_diff(line)

        print()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Examines groups of shards and identifies problems')

    parser.add_argument('-d', '--shard-dir',
                        default=CURRENT_DIR,
                        help='Root directory containing all the shards')

    args = vars(parser.parse_args())

    process_duplicates(args['shard_dir'])
